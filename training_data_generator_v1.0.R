#███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗
#██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║
#╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝
#                                                                                                                                                                  
#                  ███████╗██████╗  ██████╗    ████████╗██████╗  █████╗ ██╗███╗   ██╗██╗███╗   ██╗ ██████╗     ██████╗  █████╗ ████████╗ █████╗                    
#                  ██╔════╝██╔══██╗██╔════╝    ╚══██╔══╝██╔══██╗██╔══██╗██║████╗  ██║██║████╗  ██║██╔════╝     ██╔══██╗██╔══██╗╚══██╔══╝██╔══██╗                   
#                  ███████╗██████╔╝██║            ██║   ██████╔╝███████║██║██╔██╗ ██║██║██╔██╗ ██║██║  ███╗    ██║  ██║███████║   ██║   ███████║                   
#                  ╚════██║██╔══██╗██║            ██║   ██╔══██╗██╔══██║██║██║╚██╗██║██║██║╚██╗██║██║   ██║    ██║  ██║██╔══██║   ██║   ██╔══██║                   
#                  ███████║██████╔╝╚██████╗       ██║   ██║  ██║██║  ██║██║██║ ╚████║██║██║ ╚████║╚██████╔╝    ██████╔╝██║  ██║   ██║   ██║  ██║                   
#                  ╚══════╝╚═════╝  ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝╚═╝╚═╝  ╚═══╝ ╚═════╝     ╚═════╝ ╚═╝  ╚═╝   ╚═╝   ╚═╝  ╚═╝        
#                                                                                                                                                                  
#███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗
#██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║
#╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝

# Authored by Brodie Verrall
# Last updated: 2025-08-01

#$$$ This script generates Spatial BioCondition training datasets by generating a pool of sites from various sources, scoring them against benchmarks, and then
#$$$ assesses each site against a series of auditing criteria. Spatial datasets are available via QSpatial or TERN data portal.This workflow has several parts:

# 1) Workspace setup: creates the project directory structure and loads required libraries
# 2) Site summaries: imports and processes raw tabular data from QBEIS and QBERD to generate site summaries of BioCondition attributes
# 3) Site compiler: compiles all site summaries into a single training data pool with standardised formatting and data structure
# 4) RE assignment: checks site REs against the latest version of REDD, and changes/assigns conflicts via RE mapping, landzone  and composition data
# 5) Site scoring: scores sites against the latest version of benchmarks (including drafts), and assigns a score to each site where possible
# 6) Site assessment: assesses sites against a series of spatial criteria to ensure they meet the requirements for inclusion in the training dataset
# 7) Site auditing: selects sites based on the scoring and assessment criteria, and generates a final training dataset

#TODO: ensure all inputs are current and stored in the relevant input folder in /project_data

##### 1) Workspace setup #####
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#               ██╗       ██╗    ██╗ ██████╗ ██████╗ ██╗  ██╗███████╗██████╗  █████╗  ██████╗███████╗    ███████╗███████╗████████╗██╗   ██╗██████╗ 
#              ███║       ██║    ██║██╔═══██╗██╔══██╗██║ ██╔╝██╔════╝██╔══██╗██╔══██╗██╔════╝██╔════╝    ██╔════╝██╔════╝╚══██╔══╝██║   ██║██╔══██╗
#              ╚██║       ██║ █╗ ██║██║   ██║██████╔╝█████╔╝ ███████╗██████╔╝███████║██║     █████╗      ███████╗█████╗     ██║   ██║   ██║██████╔╝
#               ██║       ██║███╗██║██║   ██║██╔══██╗██╔═██╗ ╚════██║██╔═══╝ ██╔══██║██║     ██╔══╝      ╚════██║██╔══╝     ██║   ██║   ██║██╔═══╝ 
#               ██║██╗    ╚███╔███╔╝╚██████╔╝██║  ██║██║  ██╗███████║██║     ██║  ██║╚██████╗███████╗    ███████║███████╗   ██║   ╚██████╔╝██║     
#               ╚═╝╚═╝     ╚══╝╚══╝  ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝     ╚═╝  ╚═╝ ╚═════╝╚══════╝    ╚══════╝╚══════╝   ╚═╝    ╚═════╝ ╚═╝     
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝ 
# 1.1) Check renv status and clean workspace (optional)
# renv::status()
rm(list = ls())
gc()
save.image()

# 1.2) Install and load project library
packages <- c(
  "tidyverse",
  "data.table",
  "arrow", 
  "purrr", 
  "furrr", 
  "fs",
  "jsonlite", 
  "sf", 
  "lwgeom", 
  "terra"
)

installed_packages <- rownames(installed.packages())
for (pkg in packages) {
  if (!pkg %in% installed_packages) {
    install.packages(pkg)
  }
}
lapply(packages, library, character.only = TRUE)
rm(installed_packages, packages, pkg)
gc()

# 1.3) Set up directories and wd
# Define project root (typically getwd(), but could be set manually if needed)
project_root <- getwd()

# Create top-level folders
proj_folders <- c("archive", "project_data", "scripts")
invisible(lapply(proj_folders, function(folder) {
  path <- file.path(project_root, folder)
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
    message(sprintf("'%s' folder created.", folder))
  } else {
    message(sprintf("'%s' folder already exists.", folder))
  }
}))

# Create subfolders inside 'project_data'
subfolders <- c("training_data_inputs", "spatial_inputs", "tabular_inputs", 
                "intermediates", "outputs")
invisible(lapply(subfolders, function(sub) {
  path <- file.path(project_root, "project_data", sub)
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
    message(sprintf("'%s' folder created.", file.path("project_data", sub)))
  } else {
    message(sprintf("'%s' folder already exists.", file.path("project_data", sub)))
  }
}))

# Define full paths to subfolders for later use
spat_in_dir  <- file.path(project_root, "project_data", "spatial_inputs")
tab_in_dir   <- file.path(project_root, "project_data", "tabular_inputs")
train_in_dir <- file.path(project_root, "project_data", "training_data_inputs")
int_dir      <- file.path(project_root, "project_data", "intermediates")
out_dir      <- file.path(project_root, "project_data", "outputs")





### TODO: Intergrate spatial data in each section to pipe in and kick out only when needed
# SBC_td_v7 <-  sf::st_read("spatial_inputs/SBC_TDv7.5_WOS_BVG_STRUCTURE.gpkg", quiet = TRUE) # last version of WOS td
# pre_clear <- sf::st_read("spatial_inputs/Preclear_v13-1_2021/data.gdb", quiet = TRUE) # pre-clearance RE mapping
# remnant <- sf::st_read("spatial_inputs/Remnant_v13-1_2021/data.gdb", quiet = TRUE) # remnant vegetation mapping
# hvr <- sf::st_read("spatial_inputs/HVR_v13-1-1_2021/data.gdb", quiet = TRUE) # high value regrowth mapping
# slats_clearing <- sf::st_read("spatial_inputs/SLATS_2023/SLATS_9195/data.gdb", quiet = TRUE) #slats clearing 
# sentinel_grid <-





##### 2) Site summaries #####
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                    ██████╗        ███████╗██╗████████╗███████╗    ███████╗██╗   ██╗███╗   ███╗███╗   ███╗ █████╗ ██████╗ ██╗███████╗███████╗               
#                    ╚════██╗       ██╔════╝██║╚══██╔══╝██╔════╝    ██╔════╝██║   ██║████╗ ████║████╗ ████║██╔══██╗██╔══██╗██║██╔════╝██╔════╝               
#                     █████╔╝       ███████╗██║   ██║   █████╗      ███████╗██║   ██║██╔████╔██║██╔████╔██║███████║██████╔╝██║█████╗  ███████╗               
#                    ██╔═══╝        ╚════██║██║   ██║   ██╔══╝      ╚════██║██║   ██║██║╚██╔╝██║██║╚██╔╝██║██╔══██║██╔══██╗██║██╔══╝  ╚════██║               
#                    ███████╗██╗    ███████║██║   ██║   ███████╗    ███████║╚██████╔╝██║ ╚═╝ ██║██║ ╚═╝ ██║██║  ██║██║  ██║██║███████╗███████║               
#                    ╚══════╝╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚══════╝ ╚═════╝ ╚═╝     ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝               
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

# TODO: Pipeline in QBERD and QBEIS site summary code here, and ensure extra columns are included that include landzone, dominant floristics, composition, disturbance notes etc



##### 3) Site complier #####                                                                                                                                                           
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                        ██████╗        ███████╗██╗████████╗███████╗     ██████╗ ██████╗ ███╗   ███╗██████╗ ██╗██╗     ███████╗██████╗                           
#                        ╚════██╗       ██╔════╝██║╚══██╔══╝██╔════╝    ██╔════╝██╔═══██╗████╗ ████║██╔══██╗██║██║     ██╔════╝██╔══██╗                          
#                         █████╔╝       ███████╗██║   ██║   █████╗      ██║     ██║   ██║██╔████╔██║██████╔╝██║██║     █████╗  ██████╔╝                          
#                         ╚═══██╗       ╚════██║██║   ██║   ██╔══╝      ██║     ██║   ██║██║╚██╔╝██║██╔═══╝ ██║██║     ██╔══╝  ██╔══██╗                          
#                        ██████╔╝██╗    ███████║██║   ██║   ███████╗    ╚██████╗╚██████╔╝██║ ╚═╝ ██║██║     ██║███████╗███████╗██║  ██║                          
#                        ╚═════╝ ╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝     ╚═════╝ ╚═════╝ ╚═╝     ╚═╝╚═╝     ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝                          
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

# TODO: Import all misc training data sources and merge with site summaries from QBERD and QBEIS into TD_SITE_POOL


#   ______    ____        _______                             __           __         __         
#  |__    |  |_   |      |_     _|.--------.-----.-----.----.|  |_     .--|  |.---.-.|  |_.---.-.
#  |__    |__ _|  |_      _|   |_ |        |  _  |  _  |   _||   _|    |  _  ||  _  ||   _|  _  |
#  |______|__|______|    |_______||__|__|__|   __|_____|__|  |____|    |_____||___._||____|___._|
#                                          |__|                                                  
### TODO: Compile sites from various sources (SBC_TD_pool.csv)
# 1.4) Import training data sources
# qberd <- read.csv("training_data_inputs/QBERD_TEST.csv")
# qbeis <- read.csv("training_data_inputs/QBEIS_TEST.csv")
# rapid <- read.csv("training_data_inputs/RAPID_TEST.csv")
# tern <- read.csv("training_data_inputs/TERN_TEST.csv")
# bcc <- read.csv("training_data_inputs/BCC_TEST.csv")
# qval <- read.csv("training_data_inputs/QVAL_TEST.csv")
# quat <- read.csv("training_data_inputs/QUAT_TEST.csv")
# desktop <- read.csv("training_data_inputs/DESKTOP_TEST.csv")
# inferred <- read.csv("training_data_inputs/INFERRED_TEST.csv")


# poi <- read.csv("training_data_inputs/REv13_POI_TEST.csv") # Use this large dummy POI dataset for testing

##### 4) Regional Ecosystem Assignment #####                                                                                                                                                           
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                   ██╗  ██╗       ██████╗ ███████╗     █████╗ ███████╗███████╗██╗ ██████╗ ███╗   ██╗███╗   ███╗███████╗███╗   ██╗████████╗                     
#                   ██║  ██║       ██╔══██╗██╔════╝    ██╔══██╗██╔════╝██╔════╝██║██╔════╝ ████╗  ██║████╗ ████║██╔════╝████╗  ██║╚══██╔══╝                     
#                   ███████║       ██████╔╝█████╗      ███████║███████╗███████╗██║██║  ███╗██╔██╗ ██║██╔████╔██║█████╗  ██╔██╗ ██║   ██║                        
#                   ╚════██║       ██╔══██╗██╔══╝      ██╔══██║╚════██║╚════██║██║██║   ██║██║╚██╗██║██║╚██╔╝██║██╔══╝  ██║╚██╗██║   ██║                        
#                        ██║██╗    ██║  ██║███████╗    ██║  ██║███████║███████║██║╚██████╔╝██║ ╚████║██║ ╚═╝ ██║███████╗██║ ╚████║   ██║                        
#                        ╚═╝╚═╝    ╚═╝  ╚═╝╚══════╝    ╚═╝  ╚═╝╚══════╝╚══════╝╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═══╝   ╚═╝                        
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝


# TODO: Check REs from TD_SITE_POOL against latest version of Preclear RE mapping (RE1-RE5) to see if mapped and current. Use REDD and versioning to reassign
# TODO: Develop workflow to assign RE to sites where this data is missing or above doesn't work based on spatial intersect, landzone filtering and then match composition to remaining pool of REs

# FROM training_data_deficit_v1.0 -> calculates site thresholds based on latest version of RE mapping
#   _____    ____        _______ __ __                 __         ___ __        __ __        __   __                      __           __     __ 
#  |  |  |  |_   |      |     __|__|  |_.-----.    .--|  |.-----.'  _|__|.----.|__|  |_     |  |_|  |--.----.-----.-----.|  |--.-----.|  |.--|  |
#  |__    |_ _|  |_     |__     |  |   _|  -__|    |  _  ||  -__|   _|  ||  __||  |   _|    |   _|     |   _|  -__|__ --||     |  _  ||  ||  _  |
#     |__|__|______|    |_______|__|____|_____|    |_____||_____|__| |__||____||__|____|    |____|__|__|__| |_____|_____||__|__|_____||__||_____|
# 4.1.1) Import data
# TODO: Injest and process raw gdb and calculate area in R to replace manual QGIS step - read then write as 2d gpkg to be imported?
preclear_SBC <- read.csv(file.path(tab_in_dir, "PC_RE_v14_3577_aream2_SBCfiltered.csv")) # done in QGIS due to Z dimension in RE mapping? Layer from gdb, exported as gpkg 3577, then $area - area_m2, export csv
redd <- read.csv(file.path(tab_in_dir, "REDD_v13.1_2024.csv"))
re <- read.csv(file.path(tab_in_dir, "regional_ecosystem_2024.csv"))

# 4.1.2) Calculate preclear hectare summary and number of reference sites required
preclear_SBC_summary <- preclear_SBC %>%
  group_by(RE1) %>%
  summarise(
    CUMULATIVE_AREA_M2 = sum(area_m2, na.rm = TRUE),
    CUMULATIVE_AREA_HA = CUMULATIVE_AREA_M2 / 10000
  ) %>%
  mutate(
    REF_SITES_REQUIRED = case_when(
      CUMULATIVE_AREA_HA <= 400 ~ 1,
      CUMULATIVE_AREA_HA <= 800 ~ 2,
      CUMULATIVE_AREA_HA <= 1200 ~ 3,
      CUMULATIVE_AREA_HA <= 1600 ~ 4,
      CUMULATIVE_AREA_HA > 1600 ~ 5
    ),
    AREA_THRESHOLD = case_when(
      CUMULATIVE_AREA_HA <= 400 ~ "<=400 ha",
      CUMULATIVE_AREA_HA <= 800 ~ "400–800 ha",
      CUMULATIVE_AREA_HA <= 1200 ~ "800–1200 ha",
      CUMULATIVE_AREA_HA <= 1600 ~ "1200–1600 ha",
      CUMULATIVE_AREA_HA > 1600 ~ ">1600 ha"
    )
  ) %>%
  select(-CUMULATIVE_AREA_M2)

# 4.1.3) Join in relevant columns from REDD and RE
preclear_SBC_summary <- preclear_SBC_summary %>%
  left_join(
    re %>% select(
      NAME,
      DESCRIPTION,
      SHORT_DESCRIPTION,
      STRUCTURE_CODE,
      BVG1M,
      HABITAT,
      SOIL,
      GEOLOGY,
      EXTENT_WITHIN_PROTECTED_AREAS,
      PROTECTED_AREAS
    ),
    by = c("RE1" = "NAME")
  )

# 4.1.4) Write and clean up
# write.csv(preclear_summary, file = file.path(int_dir, "SBC_td_deficit_REv14.csv"), row.names = FALSE)
rm(preclear_SBC)
gc()




##### 5) Site scoring #####  
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                          ███████╗       ███████╗██╗████████╗███████╗    ███████╗ ██████╗ ██████╗ ██████╗ ██╗███╗   ██╗ ██████╗                               
#                          ██╔════╝       ██╔════╝██║╚══██╔══╝██╔════╝    ██╔════╝██╔════╝██╔═══██╗██╔══██╗██║████╗  ██║██╔════╝                               
#                          ███████╗       ███████╗██║   ██║   █████╗      ███████╗██║     ██║   ██║██████╔╝██║██╔██╗ ██║██║  ███╗                              
#                          ╚════██║       ╚════██║██║   ██║   ██╔══╝      ╚════██║██║     ██║   ██║██╔══██╗██║██║╚██╗██║██║   ██║                              
#                          ███████║██╗    ███████║██║   ██║   ███████╗    ███████║╚██████╗╚██████╔╝██║  ██║██║██║ ╚████║╚██████╔╝                              
#                          ╚══════╝╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚══════╝ ╚═════╝ ╚═════╝ ╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝ ╚═════╝                               
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

# TODO: Incorporate threshold and continous scoring scripts to score sites with BioCondition attributes
# This script is dependent on QBERD, QBEIS and RAPID site summaries and scoring databricks workflows. 
# Other misc training sources are stored locally and are not dynamic




##### 6) Site assessment #####  
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#               ██████╗        ███████╗██╗████████╗███████╗     █████╗ ███████╗███████╗███████╗███████╗███████╗███╗   ███╗███████╗███╗   ██╗████████╗
#              ██╔════╝        ██╔════╝██║╚══██╔══╝██╔════╝    ██╔══██╗██╔════╝██╔════╝██╔════╝██╔════╝██╔════╝████╗ ████║██╔════╝████╗  ██║╚══██╔══╝
#              ███████╗        ███████╗██║   ██║   █████╗      ███████║███████╗███████╗█████╗  ███████╗███████╗██╔████╔██║█████╗  ██╔██╗ ██║   ██║   
#              ██╔═══██╗       ╚════██║██║   ██║   ██╔══╝      ██╔══██║╚════██║╚════██║██╔══╝  ╚════██║╚════██║██║╚██╔╝██║██╔══╝  ██║╚██╗██║   ██║   
#              ╚██████╔╝██╗    ███████║██║   ██║   ███████╗    ██║  ██║███████║███████║███████╗███████║███████║██║ ╚═╝ ██║███████╗██║ ╚████║   ██║   
#               ╚═════╝ ╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚═╝  ╚═╝╚══════╝╚══════╝╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═══╝   ╚═╝   
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝                                                                                                                                                
#   ______    ____        _______                             __           __         __         
#  |    __|  |_   |      |_     _|.--------.-----.-----.----.|  |_     .--|  |.---.-.|  |_.---.-.
#  |  __  |__ _|  |_      _|   |_ |        |  _  |  _  |   _||   _|    |  _  ||  _  ||   _|  _  |
#  |______|__|______|    |_______||__|__|__|   __|_____|__|  |____|    |_____||___._||____|___._|
#                                          |__|                                                  
TD_POOL_SCORED <- read.csv(file.path(train_in_dir,"REv13_POI_TEST.csv")) # Use this large dummy POI dataset for testing

#   ______    ______      _______         __           __                                                           __              
#  |    __|  |__    |    |_     _|.---.-.|  |--.--.--.|  |.---.-.----.    .-----.----.-----.----.-----.-----.-----.|__|.-----.-----.
#  |  __  |__|    __|      |   |  |  _  ||  _  |  |  ||  ||  _  |   _|    |  _  |   _|  _  |  __|  -__|__ --|__ --||  ||     |  _  |
#  |______|__|______|      |___|  |___._||_____|_____||__||___._|__|      |   __|__| |_____|____|_____|_____|_____||__||__|__|___  |
#                                                                         |__|                                               |_____|

# 6.2.1) Add structure code to complied TD 
TD_POOL_SCORED <- TD_POOL_SCORED %>%
  left_join(re %>% select (NAME, STRUCTURE_CODE),
            by = c("RE1" = "NAME")) %>%
  relocate(STRUCTURE_CODE, .after = DBVG1M)

# 6.2.2) Add fire guidelines to complied TD
TD_POOL_SCORED <- TD_POOL_SCORED %>%
  mutate(RE1_modified = sub("^(\\d+\\.\\d+\\.\\d+).*", "\\1", RE1)) %>%
  left_join(redd %>% select(NAME, FIRE_GUIDELINES),
            by = c("RE1_modified" = "NAME")) %>%
  select(-RE1_modified) %>% 
  relocate(FIRE_GUIDELINES, .after = COLLECTION_NOTES)

# 6.2.3) Add GDA94, GDA2020 coordinates
# Define a function that processes TD_POOL_SCORED and returns it with new columns
add_gda_coords <- function(df, x_col = "x_3577", y_col = "y_3577") {
  # Convert to sf
  df_sf <- st_as_sf(df, coords = c(x_col, y_col), crs = 3577)
  
  # Extract GDA94 coords
  gda94 <- st_transform(df_sf, 4283) %>%
    mutate(
      x_gda94 = st_coordinates(.)[, 1],
      y_gda94 = st_coordinates(.)[, 2]
    ) %>%
    st_drop_geometry() %>%
    select(x_gda94, y_gda94)
  
  # Extract GDA2020 coords
  gda2020 <- st_transform(df_sf, 7844) %>%
    mutate(
      x_gda2020 = st_coordinates(.)[, 1],
      y_gda2020 = st_coordinates(.)[, 2]
    ) %>%
    st_drop_geometry() %>%
    select(x_gda2020, y_gda2020)
  
  # Bind new columns and relocate
  df <- df %>%
    bind_cols(gda94) %>%
    bind_cols(gda2020) %>%
    relocate(x_gda94, y_gda94, x_gda2020, y_gda2020, .after = ACCURACY)
  
  return(df)
}

# Apply the function (no intermediates left behind)
TD_POOL_SCORED <- add_gda_coords(TD_POOL_SCORED)
rm(add_gda_coords)

# 6.2.4) Create sf and spatial processing objects
# Ensure TD_POOL_SCORED is in the same CRS as the vector data
TD_POOL_SCORED_sf <- st_as_sf(TD_POOL_SCORED, coords = c("x_3577", "y_3577"), crs = 3577)  ### MAY HAVE TO SWAP TO 3577 

# Convert to terra vect for efficient extraction
TD_POOL_SCORED_vect <- vect(TD_POOL_SCORED_sf)

# Set chunk size (adjustable)
chunk_size <- 50000  
n_chunks <- ceiling(nrow(TD_POOL_SCORED_vect) / chunk_size)
gc()

#   ______    ______      _______               __   __         __                                               __              
#  |    __|  |__    |    |     __|.-----.---.-.|  |_|__|.---.-.|  |    .-----.----.-----.----.-----.-----.-----.|__|.-----.-----.
#  |  __  |__|__    |    |__     ||  _  |  _  ||   _|  ||  _  ||  |    |  _  |   _|  _  |  __|  -__|__ --|__ --||  ||     |  _  |
#  |______|__|______|    |_______||   __|___._||____|__||___._||__|    |   __|__| |_____|____|_____|_____|_____||__||__|__|___  |
#                                 |__|                                 |__|                                               |_____|

### 6.3.1) Regional Ecosystem Mapping
# 6.3.1.1) Import most recent RE gdb series  .....................................................................................................................


# 6.3.1.2) Sample the gdb in chunks ..............................................................................................................................


# ###QCHAT SOLUTION - UNTESTED!
# 
# library(sf)
# library(terra)
# library(dplyr)
# 
# # Path to the .gdb file
# gdb_path <- "path_to_your_file.gdb"
# 
# # List all layers in the .gdb file
# gdb_layers <- st_layers(gdb_path)$name
# print(gdb_layers)  # Inspect available layers
# 
# # Convert TD_POOL_SCORED_sf to terra vector for efficient processing
# TD_POOL_SCORED_vect <- vect(TD_POOL_SCORED_sf)
# 
# # Set chunk size
# chunk_size <- 50000
# n_chunks <- ceiling(nrow(TD_POOL_SCORED_vect) / chunk_size)
# 
# # Loop through each layer in the .gdb
# for (layer_name in gdb_layers) {
#   message("Processing layer: ", layer_name)
#   
#   # Load the current polygon layer
#   polygon_layer <- st_read(gdb_path, layer = layer_name)
#   
#   # Skip non-polygon layers (if any)
#   if (!inherits(polygon_layer, "sf") || !any(st_geometry_type(polygon_layer) %in% c("POLYGON", "MULTIPOLYGON"))) {
#     message("Skipping non-polygon layer: ", layer_name)
#     next
#   }
#   
#   # Convert polygon layer to terra format
#   polygon_vect <- vect(polygon_layer)
#   
#   # Initialise a list to store results for this layer
#   results_list <- vector("list", n_chunks)
#   
#   # Process TD_POOL_SCORED_sf in chunks
#   for (i in seq_len(n_chunks)) {
#     start_idx <- (i - 1) * chunk_size + 1
#     end_idx <- min(i * chunk_size, nrow(TD_POOL_SCORED_vect))
#     
#     # Extract chunk of TD_POOL_SCOREDnts
#     chunk <- TD_POOL_SCORED_vect[start_idx:end_idx, ]
#     
#     # Perform spatial join (intersect TD_POOL_SCOREDnts with polygons)
#     joined <- terra::intersect(chunk, polygon_vect)
#     
#     # Extract relevant attributes from the polygon layer
#     if (!is.null(joined)) {
#       joined_df <- as.data.frame(joined)
#       results_list[[i]] <- joined_df
#     }
#     
#     # Clean up
#     rm(chunk, joined)
#     gc()
#     
#     message(sprintf("  Chunk %d/%d processed for layer: %s", i, n_chunks, layer_name))
#   }
#   
#   # Combine all results for this layer into a single data frame
#   results_df <- do.call(rbind, results_list)
#   
#   # Add results to TD_POOL_SCORED_sf
#   if (!is.null(results_df) && nrow(results_df) > 0) {
#     col_prefix <- gsub("[^a-zA-Z0-9]", "_", layer_name)  # Sanitize layer name for column prefix
#     results_df <- results_df %>%
#       select(-geometry)  # Remove geometry column if present
#     colnames(results_df) <- paste0(col_prefix, "_", colnames(results_df))  # Prefix column names
#     
#     TD_POOL_SCORED_sf <- TD_POOL_SCORED_sf %>%
#       left_join(results_df, by = "fid")  # Replace with your unique ID column
#   }
#   
#   # Clean up after processing the layer
#   rm(polygon_layer, polygon_vect, results_list, results_df)
#   gc()
#   
#   message("Finished processing layer: ", layer_name)
# }
# 
# # Final clean-up
# rm(gdb_layers)
# gc()
# 
# 
# ################################################################################################################################################################
# 
# # 2.1.1) Create File Inventory
# # List all available years
# preclear_dirs <- dir_ls("spatial_inputs/Preclear", regexp = "Preclear_v\\d+-\\d+_\\d{4}")
# 
# # Extract years and paths
# preclear_years <- tibble(
#   path = preclear_dirs,
#   year = str_extract(path, "\\d{4}$")
# ) %>%
#   arrange(desc(year))  # Process newest first (often better quality)
# 
# # 2.1.2) Memory-efficient processing function
# process_preclear <- function(year_path, TD_POOL_SCOREDnts_sf) {
#   # Read with geometry repair
#   layer <- st_read(
#     file.path(year_path, "data.gdb"),
#     quiet = TRUE,
#     stringsAsFactors = FALSE,
#     promote_to_multi = TRUE  # Force multi-geometries
#   ) %>% 
#     st_cast("MULTIPOLYGON") %>%  # Convert all to supported type
#     sf::st_make_valid() %>%  # More robust than sf::st_make_valid
#     st_simplify(preserveTopology = TRUE, dTolerance = 0.1) %>%  # Reduce complexity
#     select(any_of(c("re1", "dbvg1m")))
#   
#   # Skip if no valid geometries remain
#   if (nrow(layer) == 0 || all(is.na(st_dimension(layer)))) return(NULL)
#   
#   # Spatial join with error handling
#   result <- tryCatch({
#     st_join(
#       TD_POOL_SCOREDnts_sf,
#       layer,
#       join = st_intersects,
#       left = TRUE,
#       largest = TRUE
#     ) %>%
#       st_drop_geometry() %>%
#       select(ends_with("re1"), ends_with("dbvg1m")) %>%
#       rename_with(~ paste0("preclear_", .x, "_", str_extract(year_path, "\\d{4}$")))
#   }, error = function(e) {
#     message("Failed on ", year_path, ": ", e$message)
#     NULL
#   })
#   
#   rm(layer); gc()
#   return(result)
# }
# 
# # Process sequentially (more reliable than parallel for problematic geometries)
# preclear_results <- map(
#   preclear_years$path, 
#   ~ process_preclear(., TD_POOL_SCORED_sf),
#   .progress = TRUE
# ) %>% 
#   compact() %>%
#   reduce(left_join, by = "fid")  # Adjust 'fid' to your actual ID column
# 
# # 2.1.3) Merge final results
# TD_POOL_SCORED_final <- TD_POOL_SCORED_sf %>%
#   st_drop_geometry() %>%
#   left_join(preclear_results, by = "fid")
# 
# # Force final cleanup
# rm(preclear_results); gc()


# # Ensure TD_POOL_SCORED is in the same CRS as the vector data
# TD_POOL_SCORED_sf <- st_as_sf(TD_POOL_SCORED, coords = c("x_gda94", "y_gda94"), crs = 4283) 
# 
# # Convert geometry and make valid
# pre_clear <- pre_clear %>%
#   st_cast("MULTIPOLYGON") %>%  # Convert geometry type
#   st_make_valid()  # Ensure validity
# 
# # Spatial join to extract attributes from 'preclear' at TD_POOL_SCORED locations
# TD_POOL_SCORED_sampled <- st_join(TD_POOL_SCORED_sf, pre_clear, join = st_within)  # Use st_intersects if TD_POOL_SCOREDnts can be on boundaries



#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝


### 6.3.2) High Value Regrowth Mapping
# 6.3.2.1) Import HVR gdb series  ..............................................................................................................................


# 6.3.2.2) Sample the gdb in chunks ............................................................................................................................


#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 6.3.3) Land Use Mapping 
# 6.3.3.1) Import land use data series  ........................................................................................................................

# QLD LAND USE ALUM
# SLATS LANDCOVER
# LANDSAT LANDCOVER TEMPORAL?
# HUMAN MODIFICATION INDEX?


# 6.3.3.2) Sample the gdb in chunks ............................................................................................................................


#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 6.3.4) Burn scars
# 6.3.4.1) Scrape and import burn scar rasters .................................................................................................................
# tif_files <- list.files(path = "project_data/spatial_inputs/burn_scars", pattern = "\\.tif$", full.names = TRUE, recursive = TRUE)
# 
# # 6.3.4.2) Sample the raster in chunks .........................................................................................................................
# # Process each raster file
# for(tif_file in tif_files) {
#   # Extract year from filename (assuming format like "firescar_2000.tif")
#   year <- gsub(".*?(\\d{4}).*", "\\1", basename(tif_file))
#   col_name <- paste0("FIRE_MONTH_", year)
#   message("Processing: ", basename(tif_file), " (", col_name, ")")
#   # Load raster
#   r <- rast(tif_file)
#   # Initialize result vector
#   sampled_values <- rep(NA, nrow(TD_POOL_SCORED_vect))
#   # Process in chunks
#   n_chunks <- ceiling(nrow(TD_POOL_SCORED_vect) / chunk_size)
#   for(i in seq_len(n_chunks)) {
#     start_idx <- (i - 1) * chunk_size + 1
#     end_idx <- min(i * chunk_size, nrow(TD_POOL_SCORED_vect))
#     
#     chunk <- TD_POOL_SCORED_vect[start_idx:end_idx]
#     sampled_values[start_idx:end_idx] <- terra::extract(r, chunk)[, 2]
#     
#     rm(chunk)
#     gc()
#     
#     message(sprintf("  Chunk %d/%d processed", i, n_chunks))
#   }
#   # Add results to TD_POOL_SCORED_sf
#   TD_POOL_SCORED_sf[[col_name]] <- sampled_values
#   # Clean up
#   rm(r, sampled_values)
#   gc()
# }
# rm(tif_file, tif_files)
# gc()
# 
# # Save intermediate fire sampled sites
# st_write(TD_POOL_SCORED_sf, file.path(int_dir, "TD_POOL_SCORED_fire_sampled.gpkg"), quiet = TRUE, delete_dsn = TRUE)

# 6.3.4.3) Add burn scar flag ..................................................................................................................................
# CAN IMPORT FIRE SAMPLED TD_POOL_SCORED HERE TO SKIP PROCESSING
TD_POOL_SCORED_sf <- st_read(file.path(int_dir, "TD_POOL_SCORED_fire_sampled.gpkg"), quiet = TRUE) # Load pre-sampled fire data

# Fire columns in ascending order by year
col_names <- colnames(TD_POOL_SCORED_sf)
firemonth_cols <- grep("^FIRE_MONTH_", col_names, value = TRUE)
sorted_firemonth_cols <- firemonth_cols[order(as.numeric(sub("FIRE_MONTH_", "", firemonth_cols)))]
reordered_cols <- c(setdiff(col_names, firemonth_cols), sorted_firemonth_cols)
TD_POOL_SCORED_sf <- TD_POOL_SCORED_sf[, reordered_cols]

# Remove invalid fire month columns (=!1-12)
TD_POOL_SCORED_sf <- TD_POOL_SCORED_sf %>%
  mutate(across(starts_with("FIRE_MONTH_"), ~ ifelse(. < 1 | . > 12, NA, .)))

# Improved function to process fire metrics using data.table for efficiency
process_fire_metrics <- function(TD_POOL_SCORED_sf, start_year = 1987) {
  # Drop geometry and convert to data.table
  DT <- as.data.table(st_drop_geometry(TD_POOL_SCORED_sf))
  n <- nrow(DT)
  # Extract fire month columns
  fire_cols <- grep("^FIRE_MONTH_", names(DT), value = TRUE)
  fire_years <- as.integer(str_extract(fire_cols, "\\d{4}"))
  fire_matrix <- as.matrix(DT[, ..fire_cols])
  # Valid fire months only (1–12)
  valid_mask <- fire_matrix >= 1 & fire_matrix <= 12
  valid_fire_months <- ifelse(valid_mask, fire_matrix, NA)
  # Most recent burn (fast max.col method)
  most_recent_idx <- max.col(!is.na(valid_fire_months), ties.method = "last")
  has_burn <- rowSums(!is.na(valid_fire_months)) > 0
  # Build most recent burn dates
  most_recent_burn <- rep(NA_Date_, n)
  burn_rows <- which(has_burn)
  burn_cols <- most_recent_idx[burn_rows]
  most_recent_burn[burn_rows] <- as.Date(sprintf(
    "%04d-%02d-01",
    fire_years[burn_cols],
    valid_fire_months[cbind(burn_rows, burn_cols)]
  ))
  # Compute fire indices as months since start_year
  fire_indices <- sweep(valid_fire_months, 2, (fire_years - start_year) * 12, `+`)
  # Compute fire return intervals — vectorized version
  get_intervals <- function(row, years, months) {
    idxs <- which(!is.na(row))
    if (length(idxs) > 1) {
      dates <- as.Date(sprintf("%04d-%02d-01", years[idxs], row[idxs]))
      dates <- sort(dates)
      diffs <- diff(year(dates) * 12 + month(dates))
      return(diffs)
    }
    return(NA_real_)
  }
  # Faster apply-like loop over rows
  fire_intervals <- vector("list", n)
  fire_return_mean <- numeric(n)
  for (i in seq_len(n)) {
    ints <- get_intervals(valid_fire_months[i, ], fire_years, valid_fire_months[i, ])
    fire_intervals[[i]] <- ints
    fire_return_mean[i] <- if (is.numeric(ints)) mean(ints, na.rm = TRUE) else NA_real_
  }
  # Compute collection dates and indices
  collection_date <- dmy(DT$COLLECTION_DATE)
  collection_index <- (year(collection_date) - start_year) * 12 + month(collection_date)
  # Burns after collection
  burns_post_collection <- rowSums(
    sweep(fire_indices, 1, collection_index, FUN = `>`) & !is.na(fire_indices),
    na.rm = TRUE
  )
  # Extract MIN_FIRE_INTERVAL from FIRE_GUIDELINES
  min_fire_interval <- as.numeric(str_extract(DT$FIRE_GUIDELINES, "(?<=INTERVAL_MIN: )\\d+"))
  # Add results to data.table
  DT[, `:=`(
    BURNS_1987_2024 = rowSums(!is.na(valid_fire_months)),
    fire_intervals = fire_intervals,
    MEAN_FIRE_RETURN_INTERVAL = fire_return_mean,
    collection_date = collection_date,
    collection_index = collection_index,
    BURNS_POST_COLLECTION = burns_post_collection,
    MOST_RECENT_BURN = most_recent_burn,
    MIN_FIRE_INTERVAL = min_fire_interval
  )]
  # Restore geometry and return
  TD_POOL_SCORED_sf <- st_set_geometry(as.data.frame(DT), st_geometry(TD_POOL_SCORED_sf))
  return(TD_POOL_SCORED_sf)
}

# Function to calculate fire disturbance flags using data.table for efficiency
calculate_fd_flags <- function(TD_POOL_SCORED_sf, fixed_dates) {
  # Convert to data.table
  TD <- as.data.table(st_drop_geometry(TD_POOL_SCORED_sf))
  TD[, collection_date := dmy(COLLECTION_DATE)]
  TD[, MIN_FIRE_INTERVAL_MONTHS := MIN_FIRE_INTERVAL * 12]
  n <- nrow(TD)
  # Extract FIRE columns and their years
  fire_cols <- grep("^FIRE_MONTH_", names(TD), value = TRUE)
  fire_years <- as.integer(str_extract(fire_cols, "\\d{4}"))
  fire_matrix <- as.matrix(TD[, ..fire_cols])
  # Generate burn date matrix (fast vectorized)
  fire_months_vec <- as.integer(fire_matrix)
  fire_years_mat <- matrix(rep(fire_years, each = n), nrow = n)
  valid <- !is.na(fire_months_vec) & fire_months_vec >= 1 & fire_months_vec <= 12
  # Vectorise burn dates
  burn_dates_vec <- rep(NA_Date_, length(fire_months_vec))
  burn_dates_vec[valid] <- as.Date(sprintf(
    "%04d-%02d-01",
    fire_years_mat[valid],
    fire_months_vec[valid]
  ))
  burn_dates_matrix <- matrix(burn_dates_vec, nrow = n)
  # Preallocate result matrices
  FD_FLAGS <- matrix(NA_character_, nrow = n, ncol = length(fixed_dates))
  FD_3Y <- matrix("Unburnt in assessment period or three years prior", nrow = n, ncol = length(fixed_dates))
  # Loop through fixed_dates efficiently
  for (j in seq_along(fixed_dates)) {
    fixed_date <- fixed_dates[j]
    fixed_year <- year(fixed_date)
    # Logical matrix for valid burns between collection and fixed_date
    in_range <- sweep(burn_dates_matrix, 1, TD$collection_date, `>`) &
      burn_dates_matrix <= fixed_date & !is.na(burn_dates_matrix)
    # Determine if any burn occurred
    burned_post_collection <- rowSums(in_range) > 0
    # Most recent burn dates vectorized: use `pmax` row-wise
    burn_tmp <- burn_dates_matrix
    burn_tmp[!in_range] <- as.Date(NA)
    most_recent_burn <- do.call(pmax, c(as.data.frame(burn_tmp), na.rm = TRUE))
    # Time since most recent fire
    time_to_fixed <- as.integer((year(fixed_date) - year(most_recent_burn)) * 12 +
                                  (month(fixed_date) - month(most_recent_burn)))
    time_to_fixed[is.na(most_recent_burn)] <- NA
    # Recovery status flags
    recovered  <- burned_post_collection & !is.na(time_to_fixed) & time_to_fixed > TD$MIN_FIRE_INTERVAL_MONTHS
    recovering <- burned_post_collection & !is.na(time_to_fixed) & time_to_fixed <= TD$MIN_FIRE_INTERVAL_MONTHS
    unburnt    <- !burned_post_collection
    # Set FD flags
    FD_FLAGS[recovered, j]  <- "Recovered"
    FD_FLAGS[recovering, j] <- "Recovering"
    FD_FLAGS[unburnt, j]    <- "Unburnt"
    # 3-year window check (vectorized)
    window_start <- as.Date(sprintf("%d-01-01", fixed_year - 3))
    window_end   <- as.Date(sprintf("%d-12-31", fixed_year))
    in_window <- rowSums(burn_dates_matrix >= window_start & burn_dates_matrix <= window_end, na.rm = TRUE) > 0
    FD_3Y[in_window, j] <- "Burnt in assessment period or three years prior"
  }
  # Attach results
  colnames(FD_FLAGS) <- paste0("FD_FLAG_", year(fixed_dates))
  colnames(FD_3Y) <- paste0("FD_", year(fixed_dates), "_3Y")
  # Bind results
  TD_result <- cbind(TD, FD_FLAGS, FD_3Y)
  TD_POOL_SCORED_sf <- st_set_geometry(as.data.frame(TD_result), st_geometry(TD_POOL_SCORED_sf))
  return(TD_POOL_SCORED_sf)
}

# Process fire data, calculate metrics and fire disturbance flags
system.time({TD_POOL_SCORED_sf <- process_fire_metrics(TD_POOL_SCORED_sf)})
fixed_dates <- as.Date(c("2017-01-01", "2019-01-01", "2021-01-01", "2023-01-01"))
system.time({TD_POOL_SCORED_sf <- calculate_fd_flags(TD_POOL_SCORED_sf, fixed_dates)})

# 6.3.4.4) Save intermediate fire sampled sites.................................................................................................................
TD_POOL_SCORED_sf <- TD_POOL_SCORED_sf %>%
  select(-fire_intervals)
write.csv(TD_POOL_SCORED_sf, file.path(int_dir, "TD_POOL_SCORED_fire_sampled_metrics.csv"), row.names = FALSE)

# Drop intermediate variables and clean up (optional) 
TD_POOL_SCORED_sf <- TD_POOL_SCORED_sf %>%
  select(-FIRE_GUIDELINES, -MIN_FIRE_INTERVAL, -collection_date, -collection_index, -all_of(firemonth_cols))
rm(col_names, firemonth_cols, fixed_dates, reordered_cols, sorted_firemonth_cols, process_fire_metrics, calculate_fd_flags)
gc()

#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 6.3.5) Age Since Woody Disturbance
# 6.3.5.1) Import woody disturbance product  ...................................................................................................................
slats_disturbance <- terra::rast("project_data/spatial_inputs/SLATS_woody_disturbance/DP_QLD_WOODY_AGE_SINCE_DISTURBANCE_2023.tif") # time since clearing

# 6.3.5.2) Sample the raster in chunks .........................................................................................................................
# Initialize result vector
TD_POOL_SCORED_vect <- vect(TD_POOL_SCORED_sf)
sampled_values <- rep(NA, nrow(TD_POOL_SCORED_vect))

# Process in chunks
for (i in seq_len(n_chunks)) {
  start_idx <- (i - 1) * chunk_size + 1
  end_idx <- min(i * chunk_size, nrow(TD_POOL_SCORED_vect))
  chunk <- TD_POOL_SCORED_vect[start_idx:end_idx]
  sampled_values[start_idx:end_idx] <- terra::extract(slats_disturbance, chunk)[, 2]
  rm(chunk)
  gc()
}

# Add results back to your sf object
TD_POOL_SCORED_sf$ASWD_2023 <- sampled_values

# 6.3.5.3) Add in disturbance flag .............................................................................................................................
# Initialize new columns with NA
TD_POOL_SCORED_sf$COLLECTION_YEAR <- NA_integer_
TD_POOL_SCORED_sf$WD_YEAR <- NA_integer_
TD_POOL_SCORED_sf$WD_TIMING <- NA_character_

# Process in chunks
for (i in seq_len(n_chunks)) {
  start_idx <- (i - 1) * chunk_size + 1
  end_idx <- min(i * chunk_size, nrow(TD_POOL_SCORED_sf))
  chunk <- TD_POOL_SCORED_sf[start_idx:end_idx, ]
  chunk$COLLECTION_YEAR <- as.numeric(format(
    as.Date(chunk$COLLECTION_DATE, format = "%d/%m/%Y"), 
    "%Y"))
  # Identify rows to process (sampled_value between 1-32)
  process_rows <- which(chunk$ASWD_2023 >= 1 & chunk$ASWD_2023 <= 33)
  if (length(process_rows) > 0) {
    # Calculate WD_YEAR only for valid rows
    chunk$WD_YEAR[process_rows] <- 2023 - chunk$ASWD_2023[process_rows]
    # Determine WD timing for valid rows
    chunk$WD_TIMING[process_rows] <- case_when(
      chunk$WD_YEAR[process_rows] < chunk$COLLECTION_YEAR[process_rows] ~ "Before collection",
      chunk$WD_YEAR[process_rows] == chunk$COLLECTION_YEAR[process_rows] ~ "Same year",
      chunk$WD_YEAR[process_rows] > chunk$COLLECTION_YEAR[process_rows] ~ "After collection"
    )
  }
  
  # Write results back to main sf object
  TD_POOL_SCORED_sf$COLLECTION_YEAR[start_idx:end_idx] <- chunk$COLLECTION_YEAR
  TD_POOL_SCORED_sf$WD_YEAR[start_idx:end_idx] <- chunk$WD_YEAR
  TD_POOL_SCORED_sf$WD_TIMING[start_idx:end_idx] <- chunk$WD_TIMING
  
  # Clean up
  rm(chunk)
  gc()
  
  message(sprintf("Processed chunk %d of %d (rows %d to %d) - %d valid rows", 
                  i, n_chunks, start_idx, end_idx, length(process_rows)))
}

# clean up
write.csv(TD_POOL_SCORED_sf, file.path(int_dir, "TD_POOL_SCORED_WD_sampled_metrics.csv"), row.names = FALSE)
TD_POOL_SCORED_sf <- TD_POOL_SCORED_sf %>%
  select(-COLLECTION_YEAR, -WD_YEAR)
rm(slats_disturbance, i, end_idx, process_rows, start_idx, sampled_values)
gc()

# TODO: look at efficiency improvements to function

#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 2.5) SLATS Change of Woody Vegetation
# 2.5.1) Import SLATS gdb series  ..............................................................................................................................
slats_dir <- "project_data/spatial_inputs/SLATS_clearing"
slats_folders <- list.dirs(slats_dir, full.names = TRUE, recursive = FALSE)

# 2.5.2) Sample the gdb in chunks ..............................................................................................................................


# Add a unique ID column to rejoin results
TD_POOL_SCORED_sf$JOIN_ID <- seq_len(nrow(TD_POOL_SCORED_sf))

# Loop over each SLATS folder
for (folder in slats_folders) {
  
  year_id <- basename(folder)  # e.g. "SLATS_0001"
  
  # Try to find a .gdb or .shp in the folder
  gdb_path <- list.files(folder, pattern = "\\.gdb$", full.names = TRUE)[1]
  shp_path <- list.files(folder, pattern = "\\.shp$", full.names = TRUE)[1]
  
  # Determine which format to use
  if (!is.na(gdb_path)) {
    data_path <- gdb_path
    use_layer <- NULL  # GDB handles internally
  } else if (!is.na(shp_path)) {
    data_path <- shp_path
    use_layer <- NA    # Not needed for .shp
  } else {
    warning(paste0("No .gdb or .shp found in ", folder, " — skipping."))
    next
  }
  
  cat("Processing:", year_id, "\n")
  
  # Read the SLATS layer
  slats_layer <- st_read(data_path, quiet = TRUE) %>%
    st_transform(st_crs(TD_POOL_SCORED_sf))
  
  # Handle inconsistent column names: 'descr' vs 'description'
  if ("descr" %in% names(slats_layer)) {
    slats_layer <- slats_layer %>% dplyr::rename(clearing_descr = descr)
  } else if ("description" %in% names(slats_layer)) {
    slats_layer <- slats_layer %>% dplyr::rename(clearing_descr = description)
  } else {
    warning(paste0("Skipping ", year_id, " — no 'descr' or 'description' column found."))
    next
  }
  
  # Create empty character vector to hold sampled descriptions
  sampled_descr <- rep(NA_character_, nrow(TD_POOL_SCORED_sf))
  
  # Loop over chunks
  for (i in seq_len(n_chunks)) {
    idx_start <- (i - 1) * chunk_size + 1
    idx_end <- min(i * chunk_size, nrow(TD_POOL_SCORED_sf))
    
    chunk <- TD_POOL_SCORED_sf[idx_start:idx_end, ]
    
    # Spatial join (get 'clearing_descr' text column)
    joined <- st_join(chunk, slats_layer[, "clearing_descr"], join = st_intersects, left = TRUE)
    
    # Fill values
    sampled_descr[idx_start:idx_end] <- joined$clearing_descr
  }
  
  # Add column (as character)
  col_name <- paste0(year_id, "_code")
  TD_POOL_SCORED_sf[[col_name]] <- sampled_descr
}



# TODO: look at efficiency improvements to function
# TODO: Generate SLATS flags based on clearing descriptions and collection date, clean up and rm intermediates


#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝




##### 7) Site auditing ##### 
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝  
#                              ███████╗     ███████╗██╗████████╗███████╗     █████╗ ██╗   ██╗██████╗ ██╗████████╗██╗███╗   ██╗ ██████╗ 
#                              ╚════██║     ██╔════╝██║╚══██╔══╝██╔════╝    ██╔══██╗██║   ██║██╔══██╗██║╚══██╔══╝██║████╗  ██║██╔════╝ 
#                                  ██╔╝     ███████╗██║   ██║   █████╗      ███████║██║   ██║██║  ██║██║   ██║   ██║██╔██╗ ██║██║  ███╗
#                                 ██╔╝      ╚════██║██║   ██║   ██╔══╝      ██╔══██║██║   ██║██║  ██║██║   ██║   ██║██║╚██╗██║██║   ██║
#                                 ██║██╗    ███████║██║   ██║   ███████╗    ██║  ██║╚██████╔╝██████╔╝██║   ██║   ██║██║ ╚████║╚██████╔╝
#                                 ╚═╝╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚═╝  ╚═╝ ╚═════╝ ╚═════╝ ╚═╝   ╚═╝   ╚═╝╚═╝  ╚═══╝ ╚═════╝ 
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
# ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝ 

# TODO:
# check for any evidence of disturbance post collection
# Determine if unscored sites can be considered in reference condition, and assign -1
# Generate low, mid and high strict audits

# cross reference discarded site REs with TD deficits and analogous to reprioritise












































#------------------------------------------ 2) Training Data Compiler -------------------------------------------------#
# 2.1) Mutate and join training data sources

# 2.2) Remove duplicates and ensure most recent site visit

# 2.3) Clean dataframe

# 2.4) Spatial intersect of RE and flag valid/mismatch RE

# 2.5) Create date + accuracy cuts of dataset to run through spatial auditing
# past was limited to 1995, but look at using reference condition if in PA and no disturbance
# pass accuracy was 200 m but explore higher threshold to see if anything else of value appears
# 

#------------------------------------------ 3) Training Data Compiler -------------------------------------------------#
# 3.1) Flag proximity trigger for roads, re boundary, remnant, hvr, other td points
# create 90 m buffer around all points using sentinel grid window 9x9
# intersect point assessment window with trigger layers
# look at eight surrounding 9x9 assessment windows and check triggers to suggest point relocation